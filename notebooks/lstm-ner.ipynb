{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = {'I-LOC': 0, \n",
    "        'B-ORG': 1, \n",
    "        'I-PER': 3, \n",
    "        'O': 2, \n",
    "        'I-MISC': 4, \n",
    "        'B-MISC': 5, \n",
    "        'I-ORG': 6, \n",
    "        'B-LOC': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vectors(data_file, dim=50):\n",
    "    vectors = json.load(open(data_file, 'r'))\n",
    "    vocabulary = dict([(word, wid) for wid, word in enumerate(vectors.keys())])\n",
    "    vocabulary['UUUNKKK'] = len(vocabulary)\n",
    "    wv = np.vstack([vectors.values(), np.zeros(dim)])\n",
    "    print 'Vocabulary Size: {}'.format(len(vocabulary))\n",
    "    print 'Word Vectors Size: {}'.format(wv.shape)\n",
    "    del vectors\n",
    "    return wv, vocabulary\n",
    "\n",
    "def load_sequence(data_file, vocabulary, tags, seq_len=2):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    \n",
    "    words = []\n",
    "    with open(data_file, 'r') as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if line != '' and 'DOCSTART' not in line:\n",
    "                instance = line.split(' ')\n",
    "                words.append(vocabulary[instance[0]])\n",
    "                labels.append(tags[instance[-1]])\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        instance = [words[j] if j >= 0 and j < len(words) else vocabulary['UUUNKKK'] \\\n",
    "                    for j in range(i-seq_len+1, i+1)]\n",
    "        dataset.append(instance)\n",
    "    del words\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch_data(dataset, labels, batch_size):\n",
    "    steps = int(math.ceil(len(dataset)/batch_size))\n",
    "    index = 0\n",
    "    for _ in range(steps):\n",
    "        batch_data = dataset[index: index+batch_size]\n",
    "        batch_label = labels[index: index+batch_size]\n",
    "        index += batch_size\n",
    "        yield batch_data, batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join('../data')\n",
    "wv, vocabulary = load_vectors(os.path.join(data_dir, 'conll_vectors_100.json'), dim=100)\n",
    "train_data, train_labels = load_sequence(os.path.join(data_dir, 'eng.train'), vocabulary, tags,\n",
    "                                        seq_len=5)\n",
    "valid_data, valid_labels = load_sequence(os.path.join(data_dir, 'eng.testa'), vocabulary, tags,\n",
    "                                        seq_len=5)\n",
    "print train_data[:5], train_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_len = 5\n",
    "hidden_size = 150\n",
    "learning_rate = 0.001\n",
    "l2 = 0.001\n",
    "layers = 1 \n",
    "label_size = 8\n",
    "\n",
    "label_embed = np.zeros((label_size, label_size), dtype=np.float32)\n",
    "np.fill_diagonal(label_embed, 1.0)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    input_placeholder = tf.placeholder(shape=[None, seq_len], dtype=tf.int32)\n",
    "    label_placeholder = tf.placeholder(shape=[None], dtype=tf.int64)\n",
    "    with tf.name_scope('embedding_layer'):\n",
    "        with tf.device('/cpu:0'):\n",
    "            embed = tf.nn.embedding_lookup(wv, input_placeholder)\n",
    "            window = tf.to_float(embed)\n",
    "            window = [tf.squeeze(_input, [1]) for _input in tf.split(1, seq_len, window)]\n",
    "            true_labels = tf.nn.embedding_lookup(label_embed, label_placeholder)\n",
    "    with tf.name_scope('rnn_layer'):\n",
    "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=0.0,\n",
    "                                                state_is_tuple=True, activation=tf.tanh)\n",
    "        lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, input_keep_prob=0.5,\n",
    "                                                  output_keep_prob=0.5)\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * layers, state_is_tuple=True)\n",
    "        output, state = tf.nn.rnn(cell, window, dtype=tf.float32)\n",
    "    with tf.name_scope('linear_layer'):\n",
    "        weight = tf.get_variable('W', shape=[hidden_size, label_size],\n",
    "                                initializer = tf.contrib.layers.xavier_initializer())\n",
    "        bias = tf.get_variable('bias',\n",
    "                              initializer=tf.zeros_initializer(shape=[1, label_size]))\n",
    "        logit = tf.matmul(output[-1], weight) + bias\n",
    "    with tf.name_scope('loss'):\n",
    "        ce_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logit, true_labels))\n",
    "        reg_loss = 0\n",
    "        for w in tf.trainable_variables():\n",
    "            if 'bias' not in w.name.lower():\n",
    "                reg_loss += tf.nn.l2_loss(w)\n",
    "        loss = ce_loss + l2 * reg_loss\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(ce_loss)\n",
    "    with tf.name_scope('prediction'):\n",
    "        yhat = tf.argmax(tf.nn.softmax(logit), dimension=1)\n",
    "    with tf.name_scope('accuracy'):\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(label_placeholder, yhat), dtype=tf.float32))\n",
    "    with tf.name_scope('confusion_matrix'):\n",
    "        matrix = tf.contrib.metrics.confusion_matrix(yhat, label_placeholder,\n",
    "                                                    num_classes=tf.constant(8, dtype=tf.int64),\n",
    "                                                     dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(sess):\n",
    "    costs = []\n",
    "    losses = []\n",
    "    for X_batch, y_batch in generate_batch_data(train_data, train_labels, batch_size=128):\n",
    "        _, cost, accuracy, cm = sess.run([optimizer, loss, acc, matrix], feed_dict={\n",
    "                input_placeholder: X_batch,\n",
    "                label_placeholder: y_batch\n",
    "            })\n",
    "        costs.append(cost)\n",
    "        losses.append(accuracy)\n",
    "    return np.mean(costs), np.mean(losses), cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_epochs = 500\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    sess.run(init_op)\n",
    "    for epoch in range(max_epochs):\n",
    "        avg_cost, avg_acc, cm = run_epoch(sess)\n",
    "        if epoch % 10 == 0:\n",
    "            _, cost, accuracy, cm = sess.run([optimizer, loss, acc, matrix], feed_dict={\n",
    "                input_placeholder: valid_data,\n",
    "                label_placeholder: valid_labels\n",
    "            })\n",
    "            print 'Epoch: {}'.format(epoch)\n",
    "            print 'Train Loss: {}, Train Accuracy: {}'.format(avg_cost, avg_acc)\n",
    "            print 'Valid Loss: {}, Valid Accuracy: {}'.format(cost, accuracy)\n",
    "            for tag, i in tags.items():\n",
    "                precision = cm[i, i]/np.sum(cm[i, :])\n",
    "                recall = cm[i, i]/np.sum(cm[:, i])\n",
    "                f1 = 2*precision*recall/(precision+recall)\n",
    "                print 'Tag: {}, P: {}, R: {}, F1: {}'.format(precision, recall, f1)\n",
    "    X_test, y_test = load_sequence(os.path.join(data_dir, 'eng.testb'), vocabulary, tags,\n",
    "                                        seq_len=5)\n",
    "    _, cost, accuracy = sess.run([optimizer, loss, acc, matrix], feed_dict={\n",
    "                input_placeholder: X_test,\n",
    "                label_placeholder: y_test\n",
    "            })\n",
    "    print 'Test Loss: {}, Test Accuracy: {}'.format(cost, accuracy)\n",
    "    try:\n",
    "        for tag, i in tags.items():\n",
    "            precision = cm[i, i]/np.sum(cm[i, :])\n",
    "            recall = cm[i, i]/np.sum(cm[:, i])\n",
    "            f1 = 2*precision*recall/(precision+recall)\n",
    "            print 'Tag: {}, P: {}, R: {}, F1: {}'.format(precision, recall, f1)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

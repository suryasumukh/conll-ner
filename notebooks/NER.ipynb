{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pyspark.mllib.feature import Word2Vec\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join('../data')\n",
    "conll_data = os.path.join(data_dir, 'eng.conll')\n",
    "train_data = os.path.join(data_dir, 'eng.train')\n",
    "valid_data = os.path.join(data_dir, 'eng.testa')\n",
    "test_data = os.path.join(data_dir, 'eng.testb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words..\n",
      "[u'EU', u'rejects', u'German', u'call', u'to', u'boycott', u'British', u'lamb', u'.', u'Peter', u'Blackburn', u'BRUSSELS', u'1996-08-22', u'The', u'European', u'Commission', u'said', u'on', u'Thursday', u'it']\n",
      "Vocabulary Size: 30289\n"
     ]
    }
   ],
   "source": [
    "def get_text(data_path):\n",
    "    data = (sc.textFile(conll_data)\n",
    "            .map(lambda line: line.strip())\n",
    "            .filter(lambda line: line != '' and 'DOCSTART' not in line)\n",
    "            .flatMap(lambda line: [line.split()[0]])\n",
    "           ).collect()\n",
    "    print 'Words..'\n",
    "    print data[:20]\n",
    "    return ' '.join(data)\n",
    "\n",
    "def generate_vectors(data_path, dim=50):\n",
    "    text = get_text(data_path)\n",
    "    data = (sc.parallelize([text])\n",
    "            .map(lambda line: line.split(' '))\n",
    "           )\n",
    "    model = Word2Vec().setVectorSize(dim).setMinCount(1).fit(data)\n",
    "    vectors = dict(model.getVectors())\n",
    "    for i in vectors:\n",
    "        vectors[i] = list(vectors[i])\n",
    "    with open(os.path.join(data_dir, 'conll_vectors_%s.json'%dim), 'w') as outfile:\n",
    "        outfile.write(json.dumps(vectors))\n",
    "    print 'Vocabulary Size: {}'.format(len(vectors))\n",
    "    del data\n",
    "    del vectors\n",
    "\n",
    "generate_vectors(conll_data, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'I-LOC': 0, u'B-ORG': 1, u'I-PER': 3, u'O': 2, u'I-MISC': 4, u'B-MISC': 5, u'I-ORG': 6, u'B-LOC': 7}\n",
      "Vocabulary Size: 30290\n",
      "Word Vectors Size: (30290, 300)\n"
     ]
    }
   ],
   "source": [
    "def load_vectors(data_file):\n",
    "    vectors = json.load(open(data_file, 'r'))\n",
    "    vocabulary = dict([(word, wid) for wid, word in enumerate(vectors.keys())])\n",
    "    vocabulary['UUUNKKK'] = len(vocabulary)\n",
    "    wv = np.vstack([vectors.values(), np.zeros(300)])\n",
    "    print 'Vocabulary Size: {}'.format(len(vocabulary))\n",
    "    print 'Word Vectors Size: {}'.format(wv.shape)\n",
    "    del vectors\n",
    "    return wv, vocabulary\n",
    "\n",
    "def load_data(data_file, vocabulary, tags, context_size=1):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    \n",
    "    words = []\n",
    "    with open(data_file, 'r') as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if line != '' and 'DOCSTART' not in line:\n",
    "                instance = line.split(' ')\n",
    "                words.append(vocabulary[instance[0]])\n",
    "                labels.append(tags[instance[-1]])\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        instance = [words[j] if j >= 0 and j < len(words) else vocabulary['UUUNKKK'] \\\n",
    "                    for j in range(i-context_size, i+context_size+1)]\n",
    "        dataset.append(instance)\n",
    "    del words\n",
    "    return dataset, labels\n",
    "\n",
    "def generate_batch_data(dataset, labels, batch_size):\n",
    "    steps = int(math.ceil(len(dataset)/batch_size))\n",
    "    index = 0\n",
    "    for _ in range(steps):\n",
    "        batch_data = dataset[index: index+batch_size]\n",
    "        batch_label = labels[index: index+batch_size]\n",
    "        index += batch_size\n",
    "        yield batch_data, batch_label\n",
    "\n",
    "tag_stats = dict((sc.textFile(train_data)\n",
    "             .filter(lambda line: line != '' and 'DOCSTART' not in line)\n",
    "             .map(lambda line: (line.strip().split(' ')[-1], 1))\n",
    "             .reduceByKey(lambda x,y: x+y)\n",
    "            ).collect())\n",
    "tags = dict([(t,tid) for tid,t in enumerate(tag_stats.keys())])\n",
    "print tags\n",
    "vectors, vocabulary = load_vectors(os.path.join(data_dir, 'conll_vectors.json'))\n",
    "reverse_vocabulary = dict([(v,k) for k,v in vocabulary.items()])\n",
    "X_train, y_train = load_data(train_data, vocabulary, tags, context_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:\n",
      "[30289, 27407, 24523] ['UUUNKKK', u'EU', u'rejects']\n",
      "[27407, 24523, 13654] [u'EU', u'rejects', u'German']\n",
      "[24523, 13654, 26305] [u'rejects', u'German', u'call']\n",
      "[13654, 26305, 15865] [u'German', u'call', u'to']\n",
      "[26305, 15865, 25375] [u'call', u'to', u'boycott']\n"
     ]
    }
   ],
   "source": [
    "print 'Data:'\n",
    "for i in range(5):\n",
    "    print X_train[i], map(lambda j: reverse_vocabulary[j], X_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_labels = 8\n",
    "context_size = 1\n",
    "feature_size = 300\n",
    "\n",
    "filter_sizes = [1, 2]\n",
    "\n",
    "label_encoding = np.zeros((8,8), dtype=np.float32)\n",
    "np.fill_diagonal(label_encoding, 1.0)\n",
    "\n",
    "cnn_graph = tf.Graph()\n",
    "with cnn_graph.as_default():\n",
    "    input_placeholder = tf.placeholder(shape=(None, 2*context_size+1), \n",
    "                                       dtype=tf.int32, name='data')\n",
    "    label_placeholder = tf.placeholder(shape=[None], dtype=tf.int64)\n",
    "    with tf.variable_scope('embedding'):\n",
    "        with tf.device('/cpu:0'):\n",
    "            embed = tf.nn.embedding_lookup(vectors, input_placeholder)\n",
    "            window = tf.to_float(tf.reshape(embed, shape=[-1, 2*context_size+1, \n",
    "                                                          feature_size, 1]))\n",
    "            label = tf.to_float(tf.nn.embedding_lookup(label_encoding, label_placeholder))\n",
    "    with tf.variable_scope('convolution'):\n",
    "        convolution_outputs = []\n",
    "        for filter_size in filter_sizes:\n",
    "            kernel_shape = [filter_size, feature_size, 1, 2]\n",
    "            kernel = tf.get_variable(name='kernel%s' % filter_size,\n",
    "                                        shape=kernel_shape,\n",
    "                                        initializer=tf.truncated_normal_initializer())\n",
    "            bias = tf.get_variable(name='bias%s' % filter_size,\n",
    "                                      initializer=tf.zeros_initializer(shape=[2]))\n",
    "            conv = tf.nn.conv2d(window, kernel, [1, 1, 1, 1], padding='VALID')\n",
    "            hidden = tf.nn.relu(tf.nn.bias_add(conv, bias))\n",
    "            convolution_outputs.append(hidden)\n",
    "        concat = tf.concat(1, convolution_outputs)\n",
    "        features = tf.reshape(concat, shape=[-1, 10])\n",
    "    with tf.name_scope('hidden_layers'):\n",
    "        h_window = tf.to_float(tf.reshape(embed, shape=[-1, (2*context_size+1)*feature_size]))\n",
    "        window = tf.concat(1, [features, h_window])\n",
    "        with tf.variable_scope('hidden1'):\n",
    "            weights = tf.get_variable(name='W', shape=[910, 1800],\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "            bias = tf.get_variable(name='b1', initializer=tf.zeros_initializer(shape=[1,1800]))\n",
    "            hidden1 = tf.nn.relu(tf.matmul(window, weights) + bias)\n",
    "        with tf.variable_scope('hidden2'):\n",
    "            weights = tf.get_variable(name='V', shape=[1800, 900],\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "            bias = tf.get_variable(name='b2', initializer=tf.zeros_initializer(shape=[1,900]))\n",
    "            hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + bias)\n",
    "        with tf.variable_scope('hidden3'):\n",
    "            weights = tf.get_variable(name='T', shape=[900, 300],\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "            bias = tf.get_variable(name='b3', initializer=tf.zeros_initializer(shape=[1,300]))\n",
    "            hidden3 = tf.nn.relu(tf.matmul(hidden2, weights) + bias)\n",
    "    with tf.variable_scope('linear'):\n",
    "        weight = tf.get_variable(name='U',\n",
    "                                     shape=[300, num_labels],\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "        bias = tf.get_variable(name='b',\n",
    "                                  initializer=tf.zeros_initializer(shape=[1, num_labels]))\n",
    "        logits = tf.matmul(hidden3, weight) + bias\n",
    "    with tf.variable_scope('softmax'):\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "    with tf.name_scope('loss'):\n",
    "        ce_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, label))\n",
    "        with tf.variable_scope('linear', reuse=True):\n",
    "            u = tf.get_variable(name='U')\n",
    "            u_loss = tf.nn.l2_loss(u)\n",
    "        with tf.variable_scope('hidden1', reuse=True):\n",
    "            w = tf.get_variable(name='W')\n",
    "            w_loss = tf.nn.l2_loss(w)\n",
    "        with tf.variable_scope('hidden2', reuse=True):\n",
    "            v = tf.get_variable(name='V')\n",
    "            v_loss = tf.nn.l2_loss(v)\n",
    "        with tf.variable_scope('hidden3', reuse=True):\n",
    "            t = tf.get_variable(name='T')\n",
    "            t_loss = tf.nn.l2_loss(t)\n",
    "        loss = ce_loss + 0.001 * (u_loss + w_loss + v_loss + t_loss)\n",
    "    with tf.variable_scope('Optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "    with tf.variable_scope('prediction'):\n",
    "        y_hat = tf.argmax(prediction, dimension=1)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(y_hat, label_placeholder), dtype=tf.float32))\n",
    "        confusion_matrix = tf.contrib.metrics.confusion_matrix(y_hat, label_placeholder,\n",
    "                                                              dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.573065817356, Acc: 0.863546609879\n",
      "Validation Loss:0.525649785995, Accuracy: 0.873349964619\n",
      "Tag: I-LOC, P: 0.674515247345, R:0.465138494968, F1: 0.55059359127\n",
      "Tag: B-ORG, P: nan, R:nan, F1: nan\n",
      "Tag: I-PER, P: 0.611987352371, R:0.677675426006, F1: 0.643158471368\n",
      "Tag: O, P: 0.899215936661, R:0.976332485676, F1: 0.936188848066\n",
      "Tag: I-MISC, P: 0.0, R:0.0, F1: nan\n",
      "Tag: B-MISC, P: nan, R:0.0, F1: nan\n",
      "Tag: I-ORG, P: 0.5, R:0.000956022937316, F1: 0.00190839702373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sumukh/Workspace/.virtualenvs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:30: RuntimeWarning: invalid value encountered in float_scalars\n",
      "/Users/Sumukh/Workspace/.virtualenvs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:31: RuntimeWarning: invalid value encountered in float_scalars\n",
      "/Users/Sumukh/Workspace/.virtualenvs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:32: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "X_valid, y_valid = load_data(valid_data, vocabulary, tags, context_size=1)\n",
    "with tf.Session(graph=cnn_graph) as sess:\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    sess.run(init_op)\n",
    "    for epoch in range(max_epochs):\n",
    "        costs=[]\n",
    "        accs=[]\n",
    "        for X_batch, y_batch in generate_batch_data(X_train, y_train, batch_size=128):\n",
    "            feed_dict = {\n",
    "                input_placeholder: X_batch,\n",
    "                label_placeholder: y_batch\n",
    "            }\n",
    "            _, cost, accuracy = sess.run([optimizer, loss, acc], feed_dict=feed_dict)\n",
    "            costs.append(cost)\n",
    "            accs.append(accuracy)\n",
    "        avg_cost = np.mean(costs)\n",
    "        avg_acc = np.mean(accs)\n",
    "        if epoch % 10 == 0:\n",
    "            print 'Epoch: {}, Loss: {}, Acc: {}'.format(epoch, avg_cost, avg_acc)\n",
    "            _, cost, accuracy, cm = sess.run([optimizer, loss, acc, confusion_matrix], \n",
    "                                             feed_dict={\n",
    "                    input_placeholder: X_valid,\n",
    "                    label_placeholder: y_valid\n",
    "                })\n",
    "            print 'Validation Loss:{}, Accuracy: {}'.format(cost, accuracy)\n",
    "            # print cm\n",
    "            try:\n",
    "                for tag, i in tags.items():\n",
    "                    precision = cm[i,i]/np.sum(cm[i, :], dtype=np.float32)\n",
    "                    recall = cm[i,i]/np.sum(cm[:, i], dtype=np.float32)\n",
    "                    f1 = 2*precision*recall/(precision + recall)\n",
    "                    print 'Tag: {}, P: {}, R:{}, F1: {}'.format(tag, precision, recall, f1)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

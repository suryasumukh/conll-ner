{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pyspark.mllib.feature import Word2Vec\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join('../data')\n",
    "conll_data = os.path.join(data_dir, 'eng.conll')\n",
    "train_data = os.path.join(data_dir, 'eng.train')\n",
    "valid_data = os.path.join(data_dir, 'eng.testa')\n",
    "test_data = os.path.join(data_dir, 'eng.testb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_text(data_path):\n",
    "    data = (sc.textFile(conll_data)\n",
    "            .map(lambda line: line.strip())\n",
    "            .filter(lambda line: line != '' and 'DOCSTART' not in line)\n",
    "            .flatMap(lambda line: [line.split()[0]])\n",
    "           ).collect()\n",
    "    print 'Words..'\n",
    "    print data[:20]\n",
    "    return ' '.join(data)\n",
    "\n",
    "def generate_vectors(data_path):\n",
    "    text = get_text(data_path)\n",
    "    data = (sc.parallelize([text])\n",
    "            .map(lambda line: line.split(' '))\n",
    "           )\n",
    "    model = Word2Vec().setVectorSize(300).setMinCount(1).fit(data)\n",
    "    vectors = dict(model.getVectors())\n",
    "    for i in vectors:\n",
    "        vectors[i] = list(vectors[i])\n",
    "    with open(os.path.join(data_dir, 'conll_vectors.json'), 'w') as outfile:\n",
    "        outfile.write(json.dumps(vectors))\n",
    "    print 'Vocabulary Size: {}'.format(len(vectors))\n",
    "    del data\n",
    "    del vectors\n",
    "\n",
    "generate_vectors(conll_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'I-LOC': 0, u'B-ORG': 1, u'I-PER': 3, u'O': 2, u'I-MISC': 4, u'B-MISC': 5, u'I-ORG': 6, u'B-LOC': 7}\n",
      "Vocabulary Size: 30290\n",
      "Word Vectors Size: (30290, 300)\n"
     ]
    }
   ],
   "source": [
    "def load_vectors(data_file):\n",
    "    vectors = json.load(open(data_file, 'r'))\n",
    "    vocabulary = dict([(word, wid) for wid, word in enumerate(vectors.keys())])\n",
    "    vocabulary['UUUNKKK'] = len(vocabulary)\n",
    "    wv = np.vstack([vectors.values(), np.zeros(300)])\n",
    "    print 'Vocabulary Size: {}'.format(len(vocabulary))\n",
    "    print 'Word Vectors Size: {}'.format(wv.shape)\n",
    "    del vectors\n",
    "    return wv, vocabulary\n",
    "\n",
    "def load_data(data_file, vocabulary, tags, context_size=1):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    \n",
    "    words = []\n",
    "    with open(data_file, 'r') as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if line != '' and 'DOCSTART' not in line:\n",
    "                instance = line.split(' ')\n",
    "                words.append(vocabulary[instance[0]])\n",
    "                labels.append(tags[instance[-1]])\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        instance = [words[j] if j >= 0 and j < len(words) else vocabulary['UUUNKKK'] \\\n",
    "                    for j in range(i-context_size, i+context_size+1)]\n",
    "        dataset.append(instance)\n",
    "    del words\n",
    "    return dataset, labels\n",
    "\n",
    "def generate_batch_data(dataset, labels, batch_size):\n",
    "    steps = int(math.ceil(len(dataset)/batch_size))\n",
    "    index = 0\n",
    "    for _ in range(steps):\n",
    "        batch_data = dataset[index: index+batch_size]\n",
    "        batch_label = labels[index: index+batch_size]\n",
    "        index += batch_size\n",
    "        yield batch_data, batch_label\n",
    "\n",
    "tag_stats = dict((sc.textFile(train_data)\n",
    "             .filter(lambda line: line != '' and 'DOCSTART' not in line)\n",
    "             .map(lambda line: (line.strip().split(' ')[-1], 1))\n",
    "             .reduceByKey(lambda x,y: x+y)\n",
    "            ).collect())\n",
    "tags = dict([(t,tid) for tid,t in enumerate(tag_stats.keys())])\n",
    "print tags\n",
    "vectors, vocabulary = load_vectors(os.path.join(data_dir, 'conll_vectors.json'))\n",
    "reverse_vocabulary = dict([(v,k) for k,v in vocabulary.items()])\n",
    "X_train, y_train = load_data(train_data, vocabulary, tags, context_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:\n",
      "[30289, 27407, 24523] ['UUUNKKK', u'EU', u'rejects']\n",
      "[27407, 24523, 13654] [u'EU', u'rejects', u'German']\n",
      "[24523, 13654, 26305] [u'rejects', u'German', u'call']\n",
      "[13654, 26305, 15865] [u'German', u'call', u'to']\n",
      "[26305, 15865, 25375] [u'call', u'to', u'boycott']\n"
     ]
    }
   ],
   "source": [
    "print 'Data:'\n",
    "for i in range(5):\n",
    "    print X_train[i], map(lambda j: reverse_vocabulary[j], X_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_labels = 8\n",
    "context_size = 1\n",
    "feature_size = 300\n",
    "\n",
    "filter_sizes = [1, 2]\n",
    "\n",
    "label_encoding = np.zeros((8,8), dtype=np.float32)\n",
    "np.fill_diagonal(label_encoding, 1.0)\n",
    "\n",
    "cnn_graph = tf.Graph()\n",
    "with cnn_graph.as_default():\n",
    "    input_placeholder = tf.placeholder(shape=(None, 2*context_size+1), \n",
    "                                       dtype=tf.int32, name='data')\n",
    "    label_placeholder = tf.placeholder(shape=[None], dtype=tf.int64)\n",
    "    with tf.variable_scope('embedding'):\n",
    "        with tf.device('/cpu:0'):\n",
    "            embed = tf.nn.embedding_lookup(vectors, input_placeholder)\n",
    "            window = tf.to_float(tf.reshape(embed, shape=[-1, 2*context_size+1, \n",
    "                                                          feature_size, 1]))\n",
    "            label = tf.to_float(tf.nn.embedding_lookup(label_encoding, label_placeholder))\n",
    "    with tf.variable_scope('convolution'):\n",
    "        convolution_outputs = []\n",
    "        for filter_size in filter_sizes:\n",
    "            kernel_shape = [filter_size, feature_size, 1, 2]\n",
    "            kernel = tf.get_variable(name='kernel%s' % filter_size,\n",
    "                                        shape=kernel_shape,\n",
    "                                        initializer=tf.truncated_normal_initializer())\n",
    "            bias = tf.get_variable(name='bias%s' % filter_size,\n",
    "                                      initializer=tf.zeros_initializer(shape=[2]))\n",
    "            conv = tf.nn.conv2d(window, kernel, [1, 1, 1, 1], padding='VALID')\n",
    "            hidden = tf.nn.relu(tf.nn.bias_add(conv, bias))\n",
    "            convolution_outputs.append(hidden)\n",
    "        concat = tf.concat(1, convolution_outputs)\n",
    "        features = tf.reshape(concat, shape=[-1, 10])\n",
    "    with tf.variable_scope('linear'):\n",
    "        weight = tf.get_variable(name='U',\n",
    "                                     shape=[10, num_labels],\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "        bias = tf.get_variable(name='b',\n",
    "                                  initializer=tf.zeros_initializer(shape=[1, num_labels]))\n",
    "        logits = tf.matmul(features, weight) + bias\n",
    "    with tf.variable_scope('softmax'):\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "    with tf.name_scope('loss'):\n",
    "        ce_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, label))\n",
    "        with tf.variable_scope('linear', reuse=True):\n",
    "            u = tf.get_variable(name='U')\n",
    "            u_loss = tf.nn.l2_loss(u)\n",
    "        loss = ce_loss + 0.001 * u_loss\n",
    "    with tf.variable_scope('Optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "    with tf.variable_scope('prediction'):\n",
    "        y_hat = tf.argmax(prediction, dimension=1)\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(y_hat, label_placeholder), dtype=tf.float32))\n",
    "        confusion_matrix = tf.contrib.metrics.confusion_matrix(y_hat, label_placeholder,\n",
    "                                                              dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.764626502991, Acc: 0.780827462673\n",
      "Validation Loss:0.517952561378, Accuracy: 0.836727559566\n",
      "Tag: I-LOC, P: nan, R:0.0, F1: nan\n",
      "Tag: B-ORG, P: nan, R:nan, F1: nan\n",
      "Tag: I-PER, P: 0.813953459263, R:0.0778024792671, F1: 0.14202898888\n",
      "Tag: O, P: 0.83864825964, R:0.998830676079, F1: 0.911757538067\n",
      "Tag: I-MISC, P: nan, R:0.0, F1: nan\n",
      "Tag: B-MISC, P: nan, R:0.0, F1: nan\n",
      "Tag: I-ORG, P: 0.162962958217, R:0.0105162523687, F1: 0.0197575208323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sumukh/Workspace/.virtualenvs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:30: RuntimeWarning: invalid value encountered in float_scalars\n",
      "/Users/Sumukh/Workspace/.virtualenvs/tensorflow/lib/python2.7/site-packages/ipykernel/__main__.py:31: RuntimeWarning: invalid value encountered in float_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.386577099562, Acc: 0.872361421585\n",
      "Validation Loss:0.395331829786, Accuracy: 0.866477191448\n",
      "Tag: I-LOC, P: 0.581377148628, R:0.354823291302, F1: 0.44068800775\n",
      "Tag: B-ORG, P: nan, R:nan, F1: nan\n",
      "Tag: I-PER, P: 0.763948500156, R:0.339155286551, F1: 0.469760293823\n",
      "Tag: O, P: 0.878499031067, R:0.989382326603, F1: 0.930649518442\n",
      "Tag: I-MISC, P: nan, R:0.0, F1: nan\n",
      "Tag: B-MISC, P: nan, R:0.0, F1: nan\n",
      "Tag: I-ORG, P: 0.732075452805, R:0.185468450189, F1: 0.295957286436\n",
      "Epoch: 20, Loss: 0.37018212676, Acc: 0.877476394176\n",
      "Validation Loss:0.379951834679, Accuracy: 0.872454345226\n",
      "Tag: I-LOC, P: 0.597111940384, R:0.394937932491, F1: 0.475423991573\n",
      "Tag: B-ORG, P: nan, R:nan, F1: nan\n",
      "Tag: I-PER, P: 0.792038440704, R:0.36646553874, F1: 0.501085523328\n",
      "Tag: O, P: 0.886664271355, R:0.989101707935, F1: 0.935085855034\n",
      "Tag: I-MISC, P: 0.552631556988, R:0.0166139248759, F1: 0.0322580674064\n",
      "Tag: B-MISC, P: nan, R:0.0, F1: nan\n",
      "Tag: I-ORG, P: 0.659003853798, R:0.246653914452, F1: 0.358956519508\n",
      "Epoch: 30, Loss: 0.360745877028, Acc: 0.881210684776\n",
      "Validation Loss:0.368434607983, Accuracy: 0.87625092268\n",
      "Tag: I-LOC, P: 0.632521510124, R:0.421680986881, F1: 0.506017165184\n",
      "Tag: B-ORG, P: nan, R:nan, F1: nan\n",
      "Tag: I-PER, P: 0.801298677921, R:0.391870439053, F1: 0.52633823699\n",
      "Tag: O, P: 0.890097856522, R:0.989101707935, F1: 0.936991849819\n",
      "Tag: I-MISC, P: 0.559322059155, R:0.0261075943708, F1: 0.0498866191128\n",
      "Tag: B-MISC, P: nan, R:0.0, F1: nan\n",
      "Tag: I-ORG, P: 0.66079813242, R:0.269120454788, F1: 0.382472823678\n",
      "Epoch: 40, Loss: 0.357706278563, Acc: 0.882085323334\n",
      "Validation Loss:0.365308433771, Accuracy: 0.877302289009\n",
      "Tag: I-LOC, P: 0.65192168951, R:0.429321885109, F1: 0.517708073708\n",
      "Tag: B-ORG, P: nan, R:nan, F1: nan\n",
      "Tag: I-PER, P: 0.804123699665, R:0.396316289902, F1: 0.530950884453\n",
      "Tag: O, P: 0.891056597233, R:0.989125072956, F1: 0.937533256196\n",
      "Tag: I-MISC, P: 0.50746268034, R:0.0268987342715, F1: 0.0510894058875\n",
      "Tag: B-MISC, P: nan, R:0.0, F1: nan\n",
      "Tag: I-ORG, P: 0.650723040104, R:0.279636710882, F1: 0.391173535325\n",
      "Epoch: 50, Loss: 0.355861574411, Acc: 0.882507860661\n",
      "Validation Loss:0.363523244858, Accuracy: 0.87728279829\n",
      "Tag: I-LOC, P: 0.643786132336, R:0.425501435995, F1: 0.512363445588\n",
      "Tag: B-ORG, P: nan, R:nan, F1: nan\n",
      "Tag: I-PER, P: 0.808729112148, R:0.400127023458, F1: 0.535372855172\n",
      "Tag: O, P: 0.89135748148, R:0.988937973976, F1: 0.937615661754\n",
      "Tag: I-MISC, P: 0.492753624916, R:0.0268987342715, F1: 0.0510127526358\n",
      "Tag: B-MISC, P: nan, R:0.0, F1: nan\n",
      "Tag: I-ORG, P: 0.645444571972, R:0.281070739031, F1: 0.391608373564\n",
      "Epoch: 60, Loss: 0.354532986879, Acc: 0.882773220539\n",
      "Validation Loss:0.362265050411, Accuracy: 0.877477526665\n",
      "Tag: I-LOC, P: 0.642037332058, R:0.427411645651, F1: 0.513188092244\n",
      "Tag: B-ORG, P: nan, R:nan, F1: nan\n",
      "Tag: I-PER, P: 0.808551371098, R:0.402349948883, F1: 0.537319785644\n",
      "Tag: O, P: 0.891971707344, R:0.988680720329, F1: 0.937839674311\n",
      "Tag: I-MISC, P: 0.513888895512, R:0.0292721521109, F1: 0.0553892233049\n",
      "Tag: B-MISC, P: nan, R:0.0, F1: nan\n",
      "Tag: I-ORG, P: 0.637044966221, R:0.284416824579, F1: 0.393258412569\n",
      "Epoch: 70, Loss: 0.353470027447, Acc: 0.883294045925\n",
      "Validation Loss:0.361230611801, Accuracy: 0.878061592579\n",
      "Tag: I-LOC, P: 0.645922720432, R:0.431232094765, F1: 0.517182095731\n",
      "Tag: B-ORG, P: nan, R:nan, F1: nan\n",
      "Tag: I-PER, P: 0.813894212246, R:0.405525565147, F1: 0.541331102944\n",
      "Tag: O, P: 0.892449259758, R:0.988750934601, F1: 0.93813517808\n",
      "Tag: I-MISC, P: 0.540540516376, R:0.0316455699503, F1: 0.0597907301529\n",
      "Tag: B-MISC, P: nan, R:0.0, F1: nan\n",
      "Tag: I-ORG, P: 0.633966267109, R:0.287284880877, F1: 0.395394715182\n",
      "Epoch: 80, Loss: 0.352438807487, Acc: 0.883662521839\n",
      "Validation Loss:0.360226035118, Accuracy: 0.878762483597\n",
      "Tag: I-LOC, P: 0.653763413429, R:0.435530096292, F1: 0.522785911515\n",
      "Tag: B-ORG, P: nan, R:nan, F1: nan\n",
      "Tag: I-PER, P: 0.817088603973, R:0.409971415997, F1: 0.545992779884\n",
      "Tag: O, P: 0.893010258675, R:0.988704144955, F1: 0.938423993038\n",
      "Tag: I-MISC, P: 0.552631556988, R:0.0332278497517, F1: 0.0626865687642\n",
      "Tag: B-MISC, P: nan, R:0.0, F1: nan\n",
      "Tag: I-ORG, P: 0.632989704609, R:0.293499052525, F1: 0.40104507935\n",
      "Epoch: 90, Loss: 0.351451784372, Acc: 0.884075284004\n",
      "Validation Loss:0.35894665122, Accuracy: 0.879249274731\n",
      "Tag: I-LOC, P: 0.655539751053, R:0.440783202648, F1: 0.527127329127\n",
      "Tag: B-ORG, P: nan, R:nan, F1: nan\n",
      "Tag: I-PER, P: 0.821383655071, R:0.414734840393, F1: 0.551171111687\n",
      "Tag: O, P: 0.893545508385, R:0.988774299622, F1: 0.938751035265\n",
      "Tag: I-MISC, P: 0.564102590084, R:0.0348101258278, F1: 0.0655737693287\n",
      "Tag: B-MISC, P: nan, R:0.0, F1: nan\n",
      "Tag: I-ORG, P: 0.626804113388, R:0.290630966425, F1: 0.397126064087\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 100\n",
    "X_valid, y_valid = load_data(valid_data, vocabulary, tags, context_size=1)\n",
    "with tf.Session(graph=cnn_graph) as sess:\n",
    "    init_op = tf.initialize_all_variables()\n",
    "    sess.run(init_op)\n",
    "    for epoch in range(max_epochs):\n",
    "        costs=[]\n",
    "        accs=[]\n",
    "        for X_batch, y_batch in generate_batch_data(X_train, y_train, batch_size=128):\n",
    "            feed_dict = {\n",
    "                input_placeholder: X_batch,\n",
    "                label_placeholder: y_batch\n",
    "            }\n",
    "            _, cost, accuracy = sess.run([optimizer, loss, acc], feed_dict=feed_dict)\n",
    "            costs.append(cost)\n",
    "            accs.append(accuracy)\n",
    "        avg_cost = np.mean(costs)\n",
    "        avg_acc = np.mean(accs)\n",
    "        if epoch % 10 == 0:\n",
    "            print 'Epoch: {}, Loss: {}, Acc: {}'.format(epoch, avg_cost, avg_acc)\n",
    "            _, cost, accuracy, cm = sess.run([optimizer, loss, acc, confusion_matrix], \n",
    "                                             feed_dict={\n",
    "                    input_placeholder: X_valid,\n",
    "                    label_placeholder: y_valid\n",
    "                })\n",
    "            print 'Validation Loss:{}, Accuracy: {}'.format(cost, accuracy)\n",
    "            # print cm\n",
    "            try:\n",
    "                for tag, i in tags.items():\n",
    "                    precision = cm[i,i]/np.sum(cm[i, :], dtype=np.float32)\n",
    "                    recall = cm[i,i]/np.sum(cm[:, i], dtype=np.float32)\n",
    "                    f1 = 2*precision*recall/(precision + recall)\n",
    "                    print 'Tag: {}, P: {}, R:{}, F1: {}'.format(tag, precision, recall, f1)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

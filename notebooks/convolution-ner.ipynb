{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pyspark.mllib.feature import Word2Vec\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join('../data')\n",
    "conll_data = os.path.join(data_dir, 'eng.conll')\n",
    "train_data = os.path.join(data_dir, 'eng.train')\n",
    "valid_data = os.path.join(data_dir, 'eng.testa')\n",
    "test_data = os.path.join(data_dir, 'eng.testb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vectors(data_file, dim):\n",
    "    vectors = json.load(open(data_file, 'r'))\n",
    "    vocabulary = dict([(word, wid) for wid, word in enumerate(vectors.keys())])\n",
    "    vocabulary['UUUNKKK'] = len(vocabulary)\n",
    "    wv = np.vstack([vectors.values(), np.zeros(dim)])\n",
    "    print 'Vocabulary Size: {}'.format(len(vocabulary))\n",
    "    print 'Word Vectors Size: {}'.format(wv.shape)\n",
    "    del vectors\n",
    "    return wv, vocabulary\n",
    "\n",
    "def load_chars_set(data_file):\n",
    "    char_set = json.load(open(data_file, 'r'))\n",
    "    char_map = dict([(char, cid) for cid, char in enumerate(char_set.keys())])\n",
    "    char_map['~'] = len(char_map)\n",
    "    del char_set\n",
    "    return char_map\n",
    "\n",
    "def pad(word, char_map, length, symbol='~'):\n",
    "    word += char_map[symbol]*(length - len(word))\n",
    "    return word\n",
    "\n",
    "# ToDo\n",
    "def load_data(data_file, vocabulary, tags, context_size=1):\n",
    "    pass\n",
    "\n",
    "def generate_batch_data(dataset, labels, batch_size):\n",
    "    steps = int(math.ceil(len(dataset)/batch_size))\n",
    "    index = 0\n",
    "    for _ in range(steps):\n",
    "        batch_data = dataset[index: index+batch_size]\n",
    "        batch_label = labels[index: index+batch_size]\n",
    "        index += batch_size\n",
    "        yield batch_data, batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "char_embed_size = 128\n",
    "word_embed_size = 100\n",
    "\n",
    "kernel_sizes = [2, 3, 5]\n",
    "\n",
    "label_encoding = np.zeros(shape=(8, 8), dtype=tf.float32)\n",
    "np.fill_diagonal(label_encoding, 1.0)\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    input_placeholder = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "    label_placeholder = tf.placeholder(shape=[None], dtype=tf.int64)\n",
    "    chars_placeholder = tf.placeholder(shape=[None, 61], dtype=tf.int32)\n",
    "    chars_embedding = tf.Variable(\n",
    "        tf.trunacated_normal(shape=[len(char_map), char_embed_size]),\n",
    "        trainable=True, name='char_embedding',\n",
    "        dtype=tf.float32)\n",
    "    with tf.variable_scope('embedding'):\n",
    "        with tf.device('/cpu:0'):\n",
    "            word_embedding = tf.nn.embedding_lookup(vectors, input_placeholder)\n",
    "            char_embedding = tf.nn.embedding_lookup(chars_embedding, chars_placeholder)\n",
    "            y = tf.nn.embedding_lookup(label_encoding, label_placeholder)\n",
    "            char_embed = tf.reshape(char_embedding, shape=[-1, 61, 128, 1])\n",
    "    with tf.variable_scope('conv'):\n",
    "        convolution_outputs = []\n",
    "        for kernel_size in kernel_sizes:\n",
    "            kernel_shape = [kernel_size, char_embed_size, 1, 2]\n",
    "            kernel = tf.get_variable(name='kernel%s'%kernel_size,\n",
    "                                     shape=kernel_shape,\n",
    "                                     initializer=tf.truncated_normal_initializer())\n",
    "            bias = tf.get_variable(name='bias%s'%kernel_size,\n",
    "                                   initializer=tf.zeros_initializer(shape=[2]))\n",
    "            conv = tf.nn.conv2d(char_embed, kernel, [1, 1, 1, 1], padding='VALID')\n",
    "            hidden = tf.nn.relu(tf.nn.bias_add(conv, bias))\n",
    "            convolution_outputs.append(hidden)\n",
    "        concat = tf.concat(1, convolution_outputs)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
